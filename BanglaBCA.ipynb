{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    aspect_polarity_col = next((col for col in df.columns if 'Aspect' in col and 'Polarity' in col), None)\n",
    "    if not aspect_polarity_col:\n",
    "        raise KeyError(\"Could not find the '{Aspect category, Sentiment Polarity}' column. Please verify the column names.\")\n",
    "\n",
    "    def parse_aspect_polarities(aspect_polarities_str):\n",
    "        \"\"\"\n",
    "        Parses a string containing multiple aspect-polarity pairs in the format:\n",
    "        \"{aspect1, polarity1}, {aspect2, polarity2}, ...\"\n",
    "        Returns a list of (aspect, polarity) tuples.\n",
    "        \"\"\"\n",
    "        pattern = r\"\\{([^,{}]+),\\s*([^,{}]+)\\}\"\n",
    "        matches = re.findall(pattern, aspect_polarities_str)\n",
    "        return [(aspect.strip(), polarity.strip()) for aspect, polarity in matches]\n",
    "\n",
    "    df['Aspect_Polarities_List'] = df[aspect_polarity_col].apply(parse_aspect_polarities)\n",
    "    df_exploded = df.explode('Aspect_Polarities_List').reset_index(drop=True)\n",
    "    df_exploded[['Aspect', 'Polarity']] = pd.DataFrame(df_exploded['Aspect_Polarities_List'].tolist(), index=df_exploded.index)\n",
    "    df_exploded = df_exploded.drop(columns=[aspect_polarity_col, 'Aspect_Polarities_List'])\n",
    "\n",
    "    expected_labels = {'positive', 'negative', 'neutral'}\n",
    "    initial_label_counts = df_exploded['Polarity'].value_counts()\n",
    "    print(f\"Label distribution before cleaning:\\n{initial_label_counts}\\n\")\n",
    "\n",
    "    malformed_labels = df_exploded[~df_exploded['Polarity'].isin(expected_labels)]['Polarity'].unique()\n",
    "    if len(malformed_labels) > 0:\n",
    "        print(f\"Found malformed labels: {malformed_labels}\")\n",
    "        print(\"Attempting to clean them...\\n\")\n",
    "\n",
    "        def extract_polarity(label):\n",
    "            tokens = label.split(',')\n",
    "            last_token = tokens[-1].strip()\n",
    "            if last_token in expected_labels:\n",
    "                return last_token\n",
    "            else:\n",
    "                return None \n",
    "\n",
    "        df_exploded['Clean_Polarity'] = df_exploded['Polarity'].apply(extract_polarity)\n",
    "        valid_df = df_exploded.dropna(subset=['Clean_Polarity']).copy()\n",
    "        invalid_df = df_exploded[df_exploded['Clean_Polarity'].isna()].copy()\n",
    "\n",
    "        print(f\"Dropped {len(invalid_df)} samples due to unresolvable labels.\\n\")\n",
    "\n",
    "        valid_df['Polarity'] = valid_df['Clean_Polarity']\n",
    "        valid_df = valid_df.drop(columns=['Clean_Polarity'])\n",
    "\n",
    "        cleaned_label_counts = valid_df['Polarity'].value_counts()\n",
    "        print(f\"Label distribution after cleaning:\\n{cleaned_label_counts}\\n\")\n",
    "    else:\n",
    "        valid_df = df_exploded.copy()\n",
    "        print(\"No malformed labels found.\\n\")\n",
    "\n",
    "    balanced_df = valid_df[valid_df['Polarity'] != 'neutral']\n",
    "    label_counts_after_drop = balanced_df['Polarity'].value_counts()\n",
    "    print(f\"Label distribution after dropping 'neutral':\\n{label_counts_after_drop}\\n\")\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "POS_TAGS = {\n",
    "    'PRON': set([\n",
    "        'আমি', 'তুমি', 'আপনি', 'সে', 'তারা', 'আমরা', 'উনি', 'তোমরা', 'তোমাদের',\n",
    "        'এরা', 'এইগুলি', 'সেইগুলি', 'তাহারা', 'এটা', 'এগুলো', 'আপনারা', 'আমাদের'\n",
    "    ]),\n",
    "    'DET': set([\n",
    "        'একটি', 'সেই', 'এই', 'তাই', 'কোনো', 'সব', 'এমন', 'যে', 'যত', 'প্রতিটি',\n",
    "        'কিছু', 'কিছুই', 'প্রত্যেকটি', 'অনেকগুলো', 'তিনটি', 'দুইটি', 'একত্রে',\n",
    "        'এখানে', 'সেখানে', 'খুবই', 'মোটেও'\n",
    "    ]),\n",
    "    'VERB': set([\n",
    "        'খাওয়া', 'পড়া', 'যাওয়া', 'আসা', 'করা', 'হওয়া', 'লিখা', 'বলা', 'শোনা',\n",
    "        'খাও', 'পড়', 'যাও', 'আস', 'কর', 'হয়', 'লিখ', 'বল', 'শুন',\n",
    "        'ছিলাম', 'ছিল', 'থাক', 'থাকছি', 'থাকবো', 'করবো', 'হবে',\n",
    "        'দেখা', 'শোনা', 'ভালোবাসা', 'হাসা', 'কান্না', 'নাচা', 'কাজ করা',\n",
    "        'শেখা', 'গণনা করা', 'বিস্তারিত লেখা', 'বিশ্লেষণ করা',\n",
    "        'আলোচনা করা', 'পরিচালনা করা', 'নির্মাণ করা', 'সমাধান করা', 'উন্নয়ন করা',\n",
    "        'করানো', 'নাচানো', 'দেখানো', 'শেখানো', 'লেখানো', 'নতুন', 'বজানো',\n",
    "        'শোনানো', 'পড়ানো', 'আসানো', 'লুকানো', 'বাঁচানো', 'ধোঁয়া', 'গড়ানো',\n",
    "        'বন্ধ করা', 'খেলানো', 'জীবন', 'ভালোবাসা', 'হাসা', 'কান্না',\n",
    "        'উঠানো', 'সাজানো', 'সাফ করা', 'সমাধান করা', 'উন্নয়ন করা',\n",
    "        'পরিচালনা করা', 'আলোচনা করা', 'নির্মাণ করা', 'সংরক্ষণ করা',\n",
    "        'আনন্দ করা', 'দূর করা', 'শুধু করা', 'অধ্যয়ন করা', 'আঁকা',\n",
    "        'খোলা', 'চলাচল করা',\n",
    "        # Domain-Specific Verb Suffixes\n",
    "        'ড্রাইভ করা',    # To drive\n",
    "        'কল করা',       # To call\n",
    "        'স্ক্রিন দেখা',    # To view the screen\n",
    "        'রিজিস্টার করা',  # To register\n",
    "        'অর্ডার করা',      # To order\n",
    "        'প্রচার করা',      # To promote\n",
    "        'বিজ্ঞাপন করা',    # To advertise\n",
    "        'অ্যাপডেট করা',    # To update\n",
    "        'লঞ্চ করা',        # To launch\n",
    "        'রক্ষণাবেক্ষণ করা',  # To maintain\n",
    "        'আনন্দ করা',        # To enjoy\n",
    "        'দূর করা',          # To remove\n",
    "        'শুধু করা',        # To do only\n",
    "        'অধ্যয়ন করা',      # To study\n",
    "        'আঁকা',              # To draw\n",
    "        'খোলা',              # To open\n",
    "        'চলাচল করা',        # To move\n",
    "        'সাফ করা',          # To clean\n",
    "        'কঠিনভাবে',          # To do with difficulty\n",
    "        'সহজে',              # To do easily\n",
    "        'সম্পূর্ণভাবে',      # To do completely\n",
    "        'পরিচ্ছন্নভাবে',    # To do cleanly\n",
    "        'গভীরভাবে',          # To do deeply\n",
    "        'তীক্ষ্ণভাবে',        # To do sharply\n",
    "        'অবিলম্বে'           # To do immediately\n",
    "    ]),\n",
    "    'NOUN': set([\n",
    "        'বই', 'স্কুল', 'কলেজ', 'বিশ্ববিদ্যালয়', 'ছাত্র', 'ছাত্রী', 'শিক্ষক',\n",
    "        'মানুষ', 'গাছ', 'পাখি', 'বাড়ি', 'পথ', 'মাটি', 'জল', 'আকাশ',\n",
    "        'চাকরি', 'বাজার', 'দোকান', 'রান্না', 'খেলা', 'সাহিত্য',\n",
    "        'চিঠি', 'গান', 'ছবি', 'কবিতা', 'চলচ্চিত্র', 'প্রেম', 'বন্ধুত্ব',\n",
    "        'কম্পিউটার', 'মোবাইল', 'ইন্টারনেট', 'তথ্য', 'বিজ্ঞান', 'গবেষণা',\n",
    "        'দেশ', 'শহর', 'গ্রাম', 'রাজধানী', 'বাংলাদেশ', 'ভারত', 'পাকিস্তান',\n",
    "        'পরীক্ষা', 'শিক্ষা', 'কাজ', 'পরিবার', 'সম্পর্ক', 'সময়', 'ব্যবসা',\n",
    "        'স্বাস্থ্য', 'দ্রব্য', 'পরিবেশ', 'নিয়ম', 'আইন', 'নীতি',\n",
    "        'বিজনেস', 'প্রযুক্তি', 'অর্থনীতি', 'রাজনীতি', 'মনোবিজ্ঞান', 'ভাষা',\n",
    "        'সংস্কৃতি', 'ধর্ম', 'ক্রীড়া', 'সামাজিক', 'পরিবহন', 'উদ্যোগ', 'সাহস',\n",
    "        'সমাজ', 'বিজ্ঞান', 'গবেষণা', 'নিয়ম', 'আইন', 'নীতি', 'অর্থ', 'স্বাস্থ্য',\n",
    "        'দ্রব্য', 'পরিবেশ', 'সময়', 'ব্যবসা',\n",
    "        # Domain-Specific Nouns\n",
    "        'গাড়ি', 'ফোন', 'মুভি', 'রেস্টুরেন্ট', 'মডেল', 'ইঞ্জিন', 'টেকনোলজি',\n",
    "        'ক্যামেরা', 'স্ক্রিন', 'হেডসেট', 'স্টাইল', 'সিনেমা', 'ক্যাটারিং',\n",
    "        'বেড়া', 'টেবিল', 'চেয়ার', 'রিং', 'ডিসপ্লে', 'হ্যান্ডসেট',\n",
    "        'পার্টি', 'বুট', 'ট্রান্সমিশন'\n",
    "    ]),\n",
    "    'ADJ': set([\n",
    "        'ভাল', 'মন্দ', 'বড়', 'ছোট', 'লাল', 'নীল', 'সবুজ', 'সুন্দর', 'অসুন্দর',\n",
    "        'তাড়াতাড়ি', 'ধীরে', 'শান্ত', 'গুরুত্বপূর্ণ', 'জটিল', 'সহজ', 'বিরাট', 'বিশাল',\n",
    "        'উজ্জ্বল', 'গাঢ়', 'হালকা', 'নরম', 'কঠিন', 'দৃঢ়', 'নীরব', 'উচ্চ', 'নিম্ন',\n",
    "        'তীক্ষ্ণ', 'নরমাল', 'উজ্জ্বল', 'মধুর', 'দ্রুত', 'সুস্বাদু', 'কঠোর', 'স্বচ্ছ',\n",
    "        'অস্বচ্ছ', 'শীতল', 'উষ্ণ', 'অত্যন্ত', 'সামান্য', 'মাঝারি', 'গভীর',\n",
    "        'হাই-টেক', 'অটো', 'স্মার্ট', 'প্রিমিয়াম', 'অ্যাডভান্সড', 'কাজকরি',\n",
    "        'লাইটওয়েট', 'স্টাইলিশ', 'আনন্দদায়ক', 'কমফোর্টেবল'\n",
    "    ]),\n",
    "    'ADV': set([\n",
    "        'তাড়াতাড়ি', 'ধীরে', 'ভালভাবে', 'মন্দভাবে', 'একটু', 'অনেক', 'খুব',\n",
    "        'যথেষ্ট', 'তেমন', 'মোটামুটি', 'ধীরে-ধীরে', 'আস্তে', 'যথাযথভাবে', 'বিশেষভাবে',\n",
    "        'পরিচ্ছন্নভাবে', 'সম্পূর্ণভাবে', 'অবিলম্বে', 'সহজে', 'কঠিনভাবে', 'গভীরভাবে',\n",
    "        'হাই-টেকলি', 'স্মার্টলি', 'প্রিমিয়ামলি', 'অ্যাডভান্সডলি'\n",
    "    ]),\n",
    "    'CONJ': set([\n",
    "        'এবং', 'কিন্তু', 'তবুও', 'অথবা', 'কারণ', 'যদিও', 'তাই', 'যেহেতু',\n",
    "        'তবে', 'তবেই', 'যেমন', 'যেহেতু', 'অতএব', 'নেইলে', 'অথবা', 'যদি'\n",
    "    ]),\n",
    "    'INTJ': set([\n",
    "        'আরে', 'ওহ', 'বাহ', 'আহা', 'হায়', 'ধন্যবাদ', 'কি', 'হ্যাঁ', 'না', 'ঠিক আছে',\n",
    "        'অচ্ছা', 'মারিয়া', 'ওরে', 'দয়া করে', 'দুঃখিত', 'ওই', 'হুম', 'উফ', 'আসসালামু আলাইকুম'\n",
    "    ]),\n",
    "    'PREP': set([\n",
    "        'এর', 'উপর', 'নিচে', 'পাশে', 'পরে', 'মধ্যে', 'জন্য', 'দ্বারা', 'দিকে', 'সঙ্গে',\n",
    "        'সহ', 'পর', 'অধীন', 'বাবে', 'নিম্নে', 'উর্ধ্বে', 'পর্যন্ত', 'অন্তर्गत', 'আগে',\n",
    "        'পিছনে', 'সঙ্গে', 'পাশের'\n",
    "    ]),\n",
    "    'PUNCT': set([\n",
    "        '।', ',', '.', '!', '?', ';', ':', '-', '–', '—', '(', ')', '[', ']', '{', '}', '\"', \"'\"\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Suffix-based rules for POS tagging\n",
    "NOUN_SUFFIXES = set([\n",
    "    'তা', 'ি', 'া', 'মান', 'পনা', 'কর্ম', 'গৃহ', 'পথ', 'কথা',\n",
    "    'নাম', 'গতি', 'বোধ', 'বাণী', 'প্রণয়', 'দর্শন', 'সংগঠন',\n",
    "    'উদ্দেশ্য', 'চিন্তা', 'প্রভাব', 'প্রত্যাশা', 'পরিচয়', 'ব্যবহার',\n",
    "    'বিজনেস', 'প্রযুক্তি', 'অর্থনীতি', 'রাজনীতি', 'মনোবিজ্ঞান', 'ভাষা',\n",
    "    'সংস্কৃতি', 'ধর্ম', 'ক্রীড়া', 'সামাজিক', 'পরিবহন', 'উদ্যোগ', 'সাহস',\n",
    "    'সমাজ', 'বিজ্ঞান', 'গবেষণা', 'নিয়ম', 'আইন', 'নীতি', 'অর্থ', 'স্বাস্থ্য',\n",
    "    'দ্রব্য', 'পরিবেশ', 'সময়', 'ব্যবসা',\n",
    "    # Domain-Specific Noun Suffixes\n",
    "    'গাড়ি', 'ফোন', 'মুভি', 'রেস্টুরেন্ট', 'মডেল', 'ইঞ্জিন', 'টেকনোলজি',\n",
    "    'ক্যামেরা', 'স্ক্রিন', 'হেডসেট', 'স্টাইল', 'সিনেমা', 'ক্যাটারিং',\n",
    "    'বেড়া', 'টেবিল', 'চেয়ার', 'রিং', 'ডিসপ্লে', 'হ্যান্ডসেট',\n",
    "    'পার্টি', 'বুট', 'ট্রান্সমিশন'\n",
    "])\n",
    "\n",
    "VERB_SUFFIXES = set([\n",
    "    'তে', 'া', 'চ্ছি', 'চ্ছিল', 'বে', 'ল', 'য়', 'ছে', 'ছি',\n",
    "    'নো', 'ানো', 'ইয়া', 'আনা', 'যাওয়া', 'করানো', 'নাচানো',\n",
    "    'দেখানো', 'শেখানো', 'লেখানো', 'নতুন', 'বজানো', 'শোনানো',\n",
    "    'পড়ানো', 'আসানো', 'লুকানো', 'বাঁচানো', 'ধোঁয়া', 'গড়ানো',\n",
    "    'বন্ধ করা', 'খেলানো', 'জীবন', 'ভালোবাসা', 'হাসা', 'কান্না',\n",
    "    'উঠানো', 'সাজানো', 'সাফ করা', 'সমাধান করা', 'উন্নয়ন করা',\n",
    "    'পরিচালনা করা', 'আলোচনা করা', 'নির্মাণ করা', 'সংরক্ষণ করা',\n",
    "    'আনন্দ করা', 'দূর করা', 'শুধু করা', 'অধ্যয়ন করা', 'আঁকা',\n",
    "    'খোলা', 'চলাচল করা',\n",
    "    # Domain-Specific Verb Suffixes\n",
    "    'ড্রাইভ করা',    # To drive\n",
    "    'কল করা',       # To call\n",
    "    'স্ক্রিন দেখা',    # To view the screen\n",
    "    'রিজিস্টার করা',  # To register\n",
    "    'অর্ডার করা',      # To order\n",
    "    'প্রচার করা',      # To promote\n",
    "    'বিজ্ঞাপন করা',    # To advertise\n",
    "    'অ্যাপডেট করা',    # To update\n",
    "    'লঞ্চ করা',        # To launch\n",
    "    'রক্ষণাবেক্ষণ করা',  # To maintain\n",
    "    'আনন্দ করা',        # To enjoy\n",
    "    'দূর করা',          # To remove\n",
    "    'শুধু করা',        # To do only\n",
    "    'অধ্যয়ন করা',      # To study\n",
    "    'আঁকা',              # To draw\n",
    "    'খোলা',              # To open\n",
    "    'চলাচল করা',        # To move\n",
    "    'সাফ করা',          # To clean\n",
    "    'কঠিনভাবে',          # To do with difficulty\n",
    "    'সহজে',              # To do easily\n",
    "    'সম্পূর্ণভাবে',      # To do completely\n",
    "    'পরিচ্ছন্নভাবে',    # To do cleanly\n",
    "    'গভীরভাবে',          # To do deeply\n",
    "    'তীক্ষ্ণভাবে',        # To do sharply\n",
    "    'অবিলম্বে'           # To do immediately\n",
    "])\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes a Bengali sentence by separating punctuation from words.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tokens.\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', sentence, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "def tag_word(word):\n",
    "    \"\"\"\n",
    "    Tags a single word based on predefined dictionaries and morphological suffix rules.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to tag.\n",
    "\n",
    "    Returns:\n",
    "        str: The POS tag.\n",
    "    \"\"\"\n",
    "    for pos, words in POS_TAGS.items():\n",
    "        if word in words:\n",
    "            return pos\n",
    "\n",
    "    for suffix in sorted(VERB_SUFFIXES, key=lambda x: len(x), reverse=True):\n",
    "        if word.endswith(suffix):\n",
    "            return 'VERB'\n",
    "    for suffix in sorted(NOUN_SUFFIXES, key=lambda x: len(x), reverse=True):\n",
    "        if word.endswith(suffix):\n",
    "            return 'NOUN'\n",
    "\n",
    "    return 'NOUN'\n",
    "\n",
    "def define_relations_from_pos(pos_tags):\n",
    "    \"\"\"\n",
    "    Define syntactic relations based on POS tags.\n",
    "\n",
    "    Args:\n",
    "        pos_tags (list of tuples): List containing (word, POS_tag) pairs.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (head_index, dependent_index, relation).\n",
    "    \"\"\"\n",
    "    relations = []\n",
    "    n = len(pos_tags)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(pos_tags):\n",
    "        if tag == 'VERB':\n",
    "            for j in range(i-1, -1, -1):\n",
    "                if j >= n:\n",
    "                    print(f\"Warning: j={j} exceeds the number of tokens.\")\n",
    "                    continue\n",
    "                if pos_tags[j][1] in {'PRON', 'NOUN'}:\n",
    "                    relations.append((j, i, 'subj'))\n",
    "                    break\n",
    "        if tag == 'VERB':\n",
    "            for j in range(i+1, n):\n",
    "                if j >= n:\n",
    "                    print(f\"Warning: j={j} exceeds the number of tokens.\")\n",
    "                    break\n",
    "                if pos_tags[j][1] == 'NOUN':\n",
    "                    relations.append((i, j, 'obj'))\n",
    "                    break\n",
    "        if tag == 'ADJ':\n",
    "            if i+1 < n and pos_tags[i+1][1] == 'NOUN':\n",
    "                relations.append((i+1, i, 'amod'))\n",
    "    \n",
    "    return relations\n",
    "\n",
    "def pos_tagging_from_scratch(sentence):\n",
    "    \"\"\"\n",
    "    Performs advanced POS tagging based on predefined dictionaries and morphological rules,\n",
    "    and derives syntactic relations.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence in Bengali.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: List containing (word, POS_tag) pairs.\n",
    "        list of tuples: List containing (head_index, dependent_index, relation) triples.\n",
    "    \"\"\"\n",
    "    pos_tags = []\n",
    "    relations = []\n",
    "    words = tokenize(sentence)\n",
    "    n = len(words)\n",
    "\n",
    "    if n == 0:\n",
    "        print(\"Warning: Encountered an empty sentence.\")\n",
    "        return pos_tags, relations\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        tag = tag_word(word)\n",
    "        pos_tags.append((word, tag))\n",
    "\n",
    "    relations = define_relations_from_pos(pos_tags)\n",
    "\n",
    "    if len(pos_tags) != n:\n",
    "        print(f\"Error: pos_tags length {len(pos_tags)} does not match number of words {n}\")\n",
    "        print(f\"Sentence: '{sentence}'\")\n",
    "        print(f\"Words: {words}\")\n",
    "\n",
    "    return pos_tags, relations\n",
    "\n",
    "def construct_position_graph(length):\n",
    "    adjacency_matrix = np.eye(length)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            adjacency_matrix[i][j] = 1 / (abs(i - j) + 1)\n",
    "    return adjacency_matrix\n",
    "\n",
    "def construct_semantic_similarity_graph(embeddings):\n",
    "    similarity_matrix = np.matmul(embeddings, embeddings.T)\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    similarity_matrix /= np.matmul(norms, norms.T) + 1e-8\n",
    "    return similarity_matrix\n",
    "\n",
    "def construct_syntax_graph(pos_tags, relation_weights=None):\n",
    "    \"\"\"\n",
    "    Constructs a syntax adjacency matrix based on POS tags.\n",
    "\n",
    "    Args:\n",
    "        pos_tags (list of tuples): List containing (word, POS_tag) pairs.\n",
    "        relation_weights (dict, optional): Weights for different relation types.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Syntax adjacency matrix.\n",
    "    \"\"\"\n",
    "    if relation_weights is None:\n",
    "        relation_weights = {'subj': 1.0, 'obj': 1.0, 'amod': 1.0}\n",
    "\n",
    "    relations = define_relations_from_pos(pos_tags)\n",
    "    n = len(pos_tags)\n",
    "    adjacency_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for (head, dependent, relation) in relations:\n",
    "        adjacency_matrix[head][dependent] = relation_weights.get(relation, 1.0)\n",
    "    \n",
    "    return adjacency_matrix\n",
    "\n",
    "class BidirectionalCrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BidirectionalCrossAttention, self).__init__()\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, bert_embeddings, gcn_embeddings):\n",
    "        query = self.query_proj(bert_embeddings) \n",
    "        key = self.key_proj(gcn_embeddings)   \n",
    "        value = self.value_proj(gcn_embeddings)  \n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) \n",
    "        attention_weights = self.softmax(attention_scores)      \n",
    "\n",
    "        attended_gcn = torch.matmul(attention_weights, value)  \n",
    "\n",
    "        return attended_gcn\n",
    "\n",
    "class HighwayNetworkGate(nn.Module):\n",
    "    def __init__(self, input_dim, gate_dim):\n",
    "        super(HighwayNetworkGate, self).__init__()\n",
    "        self.H = nn.Sequential(\n",
    "            nn.Linear(input_dim, gate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_dim, input_dim)\n",
    "        )\n",
    "        self.T = nn.Sequential(\n",
    "            nn.Linear(input_dim, gate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Hx = self.H(x)       \n",
    "        Tx = self.T(x)        \n",
    "        Cx = 1 - Tx            \n",
    "        output = Tx * Hx + Cx * x\n",
    "        return output\n",
    "\n",
    "def compute_dynamic_adjacency(pos_tags, max_length):\n",
    "    n = min(len(pos_tags), max_length)\n",
    "    pos_tags = pos_tags[:n]\n",
    "\n",
    "    syntax_matrix = construct_syntax_graph(pos_tags)\n",
    "    position_matrix = construct_position_graph(n)\n",
    "    combined_matrix = syntax_matrix + position_matrix[:n, :n]\n",
    "    max_val = combined_matrix.max()\n",
    "    if max_val != 0:\n",
    "        combined_matrix = combined_matrix / max_val\n",
    "\n",
    "    padded_matrix = np.zeros((max_length, max_length))\n",
    "    padded_matrix[:n, :n] = combined_matrix\n",
    "\n",
    "    return torch.tensor(padded_matrix, dtype=torch.float)\n",
    "\n",
    "def format_input(comment, aspect):\n",
    "    return f\"Comment: {comment} [SEP] Aspect: {aspect}\"\n",
    "\n",
    "\n",
    "class ABSA_Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label2id = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.df.iloc[idx]['Comment']\n",
    "        aspect = self.df.iloc[idx]['Aspect']\n",
    "        polarity = self.df.iloc[idx]['Polarity']\n",
    "\n",
    "        formatted_input = format_input(comment, aspect)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text=formatted_input,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        aspect_tokens = self.tokenizer.tokenize(aspect)\n",
    "        aspect_token_ids = self.tokenizer.convert_tokens_to_ids(aspect_tokens)\n",
    "        input_id_list = input_ids.tolist()\n",
    "\n",
    "        aspect_position = 0 \n",
    "        for i in range(len(input_id_list)):\n",
    "            if input_id_list[i:i+len(aspect_token_ids)] == aspect_token_ids:\n",
    "                aspect_position = i\n",
    "                break\n",
    "\n",
    "        pos_tags, relations = pos_tagging_from_scratch(comment)\n",
    "        dynamic_adjacency = compute_dynamic_adjacency(pos_tags, self.max_length)  \n",
    "\n",
    "        label = self.label2id.get(polarity.lower(), None)\n",
    "\n",
    "        if label is None:\n",
    "            label = self.label2id['neutral']\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'dynamic_adjacency': dynamic_adjacency,\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'aspect_position': torch.tensor(aspect_position, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class Hybrid_Model(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-multilingual-cased', hidden_dim=768, num_classes=3, max_length=128):\n",
    "        super(Hybrid_Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        self.gcn_syntax = GATConv(in_channels=hidden_dim, out_channels=hidden_dim, heads=1, concat=False)\n",
    "        self.gcn_semantic = GATConv(in_channels=hidden_dim, out_channels=hidden_dim, heads=1, concat=False)\n",
    "        \n",
    "        self.cross_attention_syntax = BidirectionalCrossAttention(hidden_dim)\n",
    "        self.cross_attention_semantic = BidirectionalCrossAttention(hidden_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim*2, nhead=8, dim_feedforward=2048, dropout=0.1)\n",
    "        self.mambaformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.gcn_aspect = GATConv(in_channels=hidden_dim, out_channels=hidden_dim, heads=1, concat=False)\n",
    "        \n",
    "        self.kangate = HighwayNetworkGate(input_dim=hidden_dim*3, gate_dim=hidden_dim)  \n",
    "        self.classification_head = nn.Linear(hidden_dim * 4, num_classes)  \n",
    "\n",
    "        self.max_length = max_length  \n",
    "\n",
    "    def adjacency_to_edge_index(self, adjacency_matrix):\n",
    "        \"\"\"\n",
    "        Convert an adjacency matrix to edge_index and edge_attr for PyTorch Geometric.\n",
    "        \"\"\"\n",
    "        edge_index, edge_attr = dense_to_sparse(adjacency_matrix)\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, adjacency_matrices, aspect_positions):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        H = bert_outputs.last_hidden_state \n",
    "\n",
    "        batch_size, seq_len, hidden_dim = H.size()\n",
    "        aspect_embeddings = []\n",
    "        for i in range(batch_size):\n",
    "            pos = aspect_positions[i]\n",
    "            if pos >= seq_len:\n",
    "                pos = 0  \n",
    "            aspect_emb = H[i, pos, :]  \n",
    "            aspect_embeddings.append(aspect_emb)\n",
    "        aspect_embeddings = torch.stack(aspect_embeddings, dim=0)\n",
    "\n",
    "        semantic_adjacency_matrices = []\n",
    "        for i in range(batch_size):\n",
    "            H_i = H[i].detach().cpu().numpy()  \n",
    "            semantic_matrix = construct_semantic_similarity_graph(H_i)\n",
    "            if semantic_matrix.max() > 0:\n",
    "                semantic_matrix = semantic_matrix / semantic_matrix.max()\n",
    "            padded_semantic = np.zeros((self.max_length, self.max_length))\n",
    "            current_len = min(H_i.shape[0], self.max_length)\n",
    "            padded_semantic[:current_len, :current_len] = semantic_matrix[:self.max_length, :self.max_length]\n",
    "            semantic_adjacency_matrices.append(torch.tensor(padded_semantic, dtype=torch.float))\n",
    "        semantic_adjacency_matrices = torch.stack(semantic_adjacency_matrices, dim=0).to(input_ids.device) \n",
    "\n",
    "        syntax_adjacency = adjacency_matrices  \n",
    "        semantic_adjacency = semantic_adjacency_matrices  \n",
    "\n",
    "        H_gcn_syntax_list = []\n",
    "        for i in range(batch_size):\n",
    "            A_syntax = syntax_adjacency[i]\n",
    "            edge_index_syntax, edge_attr_syntax = self.adjacency_to_edge_index(A_syntax)\n",
    "            if edge_index_syntax.numel() == 0:\n",
    "                H_gcn_syntax = H[i]\n",
    "            else:\n",
    "                H_gcn_syntax = self.gcn_syntax(H[i], edge_index_syntax, edge_attr_syntax)\n",
    "            H_gcn_syntax_list.append(H_gcn_syntax)\n",
    "\n",
    "        H_gcn_syntax = torch.stack(H_gcn_syntax_list, dim=0)  \n",
    "\n",
    "        H_gcn_semantic_list = []\n",
    "        for i in range(batch_size):\n",
    "            A_semantic = semantic_adjacency[i]\n",
    "            edge_index_semantic, edge_attr_semantic = self.adjacency_to_edge_index(A_semantic)\n",
    "            if edge_index_semantic.numel() == 0:\n",
    "                H_gcn_semantic = H[i]\n",
    "            else:\n",
    "                H_gcn_semantic = self.gcn_semantic(H[i], edge_index_semantic, edge_attr_semantic)\n",
    "            H_gcn_semantic_list.append(H_gcn_semantic)\n",
    "\n",
    "        H_gcn_semantic = torch.stack(H_gcn_semantic_list, dim=0) \n",
    "        attention_output_syntax = self.cross_attention_syntax(H, H_gcn_syntax)  \n",
    "\n",
    "        attention_output_semantic = self.cross_attention_semantic(H, H_gcn_semantic)  \n",
    "        concatenated_attentions = torch.cat((attention_output_syntax, attention_output_semantic), dim=-1) \n",
    "\n",
    "        concatenated_attentions = concatenated_attentions.permute(1, 0, 2)\n",
    "        mambaformer_output = self.mambaformer(concatenated_attentions)  \n",
    "        mambaformer_output = mambaformer_output.permute(1, 0, 2)\n",
    "\n",
    "        H_gcn_aspect_list = []\n",
    "        for i in range(batch_size):\n",
    "            A_aspect = adjacency_matrices[i]\n",
    "            edge_index_aspect, edge_attr_aspect = self.adjacency_to_edge_index(A_aspect)\n",
    "            if edge_index_aspect.numel() == 0:\n",
    "                H_gcn_aspect = H[i]\n",
    "            else:\n",
    "                H_gcn_aspect = self.gcn_aspect(H[i], edge_index_aspect, edge_attr_aspect)\n",
    "            H_gcn_aspect_list.append(H_gcn_aspect)\n",
    "\n",
    "        H_gcn_aspect = torch.stack(H_gcn_aspect_list, dim=0) \n",
    "\n",
    "        mambaformer_pooled = mambaformer_output.mean(dim=1) \n",
    "        gcn_aspect_pooled = H_gcn_aspect.mean(dim=1)       \n",
    "\n",
    "        combined_features = torch.cat((mambaformer_pooled, gcn_aspect_pooled), dim=1) \n",
    "        gated_features = self.kangate(combined_features)\n",
    "        final_representation = torch.cat((gated_features, aspect_embeddings), dim=1)  \n",
    "\n",
    "        logits = self.classification_head(final_representation) \n",
    "\n",
    "        return logits\n",
    "\n",
    "    def set_max_length(self, max_length):\n",
    "        self.max_length = max_length\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        dynamic_adjacency = batch['dynamic_adjacency'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        aspect_positions = batch['aspect_position'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, dynamic_adjacency, aspect_positions)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        batch_correct = (preds == labels).sum().item()\n",
    "        total_correct += batch_correct\n",
    "        total_examples += labels.size(0)\n",
    "\n",
    "        batch_acc = batch_correct / labels.size(0)\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'acc': f\"{batch_acc:.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_examples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            dynamic_adjacency = batch['dynamic_adjacency'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            aspect_positions = batch['aspect_position'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, dynamic_adjacency, aspect_positions)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            batch_correct = (predictions == labels).sum().item()\n",
    "            total_correct += batch_correct\n",
    "            total_examples += labels.size(0)\n",
    "\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            batch_acc = batch_correct / labels.size(0)\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'acc': f\"{batch_acc:.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_examples\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted')\n",
    "    return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "def main():\n",
    "    DATA_PATH = r\"E:\\Bengali Aspect\\BANGLA_ABSA dataset\\Restauant\\Restaurant_ABSA.xlsx\"\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 20\n",
    "    LEARNING_RATE = 2e-5\n",
    "    MAX_LEN = 128\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data(DATA_PATH)\n",
    "    train_df, val_df, test_df = np.split(df.sample(frac=1, random_state=42),\n",
    "                                         [int(.8*len(df)), int(.9*len(df))])\n",
    "    \n",
    "    print(f\"Number of training samples: {len(train_df)}\")\n",
    "    print(\"Sample training data:\")\n",
    "    print(train_df.head(), \"\\n\")\n",
    "    \n",
    "    print(f\"Number of validation samples: {len(val_df)}\")\n",
    "    print(\"Sample validation data:\")\n",
    "    print(val_df.head(), \"\\n\")\n",
    "    \n",
    "    print(f\"Number of test samples: {len(test_df)}\")\n",
    "    print(\"Sample test data:\")\n",
    "    print(test_df.head(), \"\\n\")\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = ABSA_Dataset(train_df, tokenizer, max_length=MAX_LEN)\n",
    "    val_dataset = ABSA_Dataset(val_df, tokenizer, max_length=MAX_LEN)\n",
    "    test_dataset = ABSA_Dataset(test_df, tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    model = Hybrid_Model(bert_model_name='bert-base-multilingual-cased', hidden_dim=768, num_classes=3, max_length=MAX_LEN)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, DEVICE)\n",
    "        val_metrics = eval_model(model, val_loader, DEVICE)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Validation Loss: {val_metrics['loss']:.4f}, Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Validation Metrics: Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            torch.save(model.state_dict(), \"best_model_double_graph_enhanced.pth\")\n",
    "            print(\"Saved the best double graph enhanced model!\")\n",
    "            patience_counter = 0 \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in F1. Patience counter: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    print(\"\\nEvaluating on Test Data...\")\n",
    "    model.load_state_dict(torch.load(\"best_model_double_graph_enhanced.pth\"))\n",
    "    test_metrics = eval_model(model, test_loader, DEVICE)\n",
    "\n",
    "    print(f\"Test Loss: {test_metrics['loss']:.4f}, Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test Metrics: Precision: {test_metrics['precision']:.4f}, Recall: {test_metrics['recall']:.4f}, F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
